{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "name": "",
  "signature": "sha256:4977f61fa8565215b680c3db8b5c5719dd9e78711aa102e5976ec03bd9e473dd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "0dbbf856ba02bf28683951344311141c8ddb3841"
     },
     "source": [
      "# Introduction\n",
      "A while ago I tried to predict the sentiment of tweets in [another Kaggle kernel](https://www.kaggle.com/bertcarremans/predicting-sentiment-with-text-features) by using the text and basic classifers. In this notebook I want to try whether we can outperform these models with a **deep learning model**.\n",
      "\n",
      "\n",
      "We'll do the following:\n",
      "* fit a deep learning model with [Keras](https://keras.io/)\n",
      "* identify and deal with overfitting\n",
      "* use word embeddings\n",
      "* build on a pretrained model\n",
      "\n",
      "I hope you'll enjoy this notebook and perhaps it might help you in one of your personal projects. Have fun!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "0f61e8a86efa85dbf7e7f77719b6547351b950c1"
     },
     "source": [
      "# Set-up of the project\n",
      "We'll start by importing some packages."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# Basic packages\n",
      "import pandas as pd \n",
      "import numpy as np\n",
      "import re\n",
      "import collections\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Packages for data preparation\n",
      "from sklearn.model_selection import train_test_split\n",
      "from nltk.corpus import stopwords\n",
      "from keras.preprocessing.text import Tokenizer\n",
      "from keras.utils.np_utils import to_categorical\n",
      "from sklearn.preprocessing import LabelEncoder\n",
      "\n",
      "# Packages for modeling\n",
      "from keras import models\n",
      "from keras import layers\n",
      "from keras import regularizers"
     ],
     "language": "python",
     "metadata": {
      "_cell_guid": "21cee267-f257-4ba3-88e1-d22e2fc57c7e",
      "_uuid": "2fec7805d8d0e999fc405d136beff46ac331b963"
     },
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "5c8c473a857b9fd044c11bb332c351df0a2dd9fe"
     },
     "source": [
      "Then we set some parameters that will be used throughout the notebook."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
      "VAL_SIZE = 1000  # Size of the validation set\n",
      "NB_START_EPOCHS = 20  # Number of epochs we usually start to train with\n",
      "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "992184fc16ab7ce9917146d08581772c2349337d"
     },
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "66b54d1ea619e1067f1d39824b103bd060369885"
     },
     "source": [
      "We read in the csv with the tweets data and perform a random shuffle. It's a good practice to shuffle the data before splitting between a train and test set. That way the sentiment classes are equally distributed over the train and test sets.\n",
      "\n",
      "We'll only keep the *text* column as input and the *airline_sentiment* column as the target. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\n",
      "df = pd.read_csv('Tweets.csv')\n",
      "df = df.reindex(np.random.permutation(df.index))  \n",
      "df = df[['text', 'airline_sentiment']]\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "ca3edd68b979a54e032b5eaa62d0ea1dc5caff66"
     },
     "outputs": [
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>text</th>\n",
        "      <th>airline_sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>9882</th>\n",
        "      <td>@USAirways No flights out of Philly because of...</td>\n",
        "      <td>negative</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5793</th>\n",
        "      <td>@SouthwestAir a nice trip back home after a lo...</td>\n",
        "      <td>positive</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9881</th>\n",
        "      <td>@USAirways does anyone actually work at the di...</td>\n",
        "      <td>negative</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6887</th>\n",
        "      <td>@JetBlue absolutely no worries. I'm just never...</td>\n",
        "      <td>negative</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10132</th>\n",
        "      <td>@USAirways @AmericanAir my friends are strande...</td>\n",
        "      <td>negative</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "                                                    text airline_sentiment\n",
        "9882   @USAirways No flights out of Philly because of...          negative\n",
        "5793   @SouthwestAir a nice trip back home after a lo...          positive\n",
        "9881   @USAirways does anyone actually work at the di...          negative\n",
        "6887   @JetBlue absolutely no worries. I'm just never...          negative\n",
        "10132  @USAirways @AmericanAir my friends are strande...          negative"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "_uuid": "d04f62e6a9c95c9d8e05b33b1033798d41bd0ab3"
     },
     "source": [
      "Data preparation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "2b753fcb54745450486fa917de3983ae27a62908"
     },
     "source": [
      "### Data cleaning\n",
      "The first thing we'll do is removing stopwords. These words do not have any value for predicting the sentiment. Furthermore, as we want to build a model that can be used for other airline companies as well, we remove the mentions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def remove_stopwords(input_text):\n",
      "        stopwords_list = stopwords.words('english')\n",
      "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
      "        whitelist = [\"n't\", \"not\", \"no\"]\n",
      "        words = input_text.split() \n",
      "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
      "        return \" \".join(clean_words) \n",
      "    \n",
      "def remove_mentions(input_text):\n",
      "        return re.sub(r'@\\w+', '', input_text)\n",
      "       \n",
      "df.text = df.text.apply(remove_stopwords).apply(remove_mentions)\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "c1210dcf7d75e9cfd96d0ca489c7b9136aad21db"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "-c:6: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
       ]
      },
      {
       "html": [
        "<div style=\"max-width:1500px;overflow:auto;\">\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
        "        vertical-align: middle;\n",
        "    }\n",
        "\n",
        "    .dataframe tbody tr th {\n",
        "        vertical-align: top;\n",
        "    }\n",
        "\n",
        "    .dataframe thead th {\n",
        "        text-align: right;\n",
        "    }\n",
        "</style>\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>text</th>\n",
        "      <th>airline_sentiment</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>9882</th>\n",
        "      <td>No flights Philly system wide tech issue.</td>\n",
        "      <td>negative</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5793</th>\n",
        "      <td>nice trip back home looong vaca \ud83d\ude0a\ud83c\udf34 http://t.c...</td>\n",
        "      <td>positive</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9881</th>\n",
        "      <td>anyone actually work dividend miles departmen...</td>\n",
        "      <td>negative</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6887</th>\n",
        "      <td>absolutely no worries. I'm never flying again...</td>\n",
        "      <td>negative</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10132</th>\n",
        "      <td>friends stranded KPHL representatives even g...</td>\n",
        "      <td>negative</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "                                                    text airline_sentiment\n",
        "9882          No flights Philly system wide tech issue.           negative\n",
        "5793    nice trip back home looong vaca \ud83d\ude0a\ud83c\udf34 http://t.c...          positive\n",
        "9881    anyone actually work dividend miles departmen...          negative\n",
        "6887    absolutely no worries. I'm never flying again...          negative\n",
        "10132    friends stranded KPHL representatives even g...          negative"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "d2ea26c0cc56f3d456f6e06df7d66e91d6476e16"
     },
     "source": [
      "### Train-Test split\n",
      "The evaluation of the model performance needs to be done on a separate test set. As such, we can estimate how well the model generalizes. This is done with the *train_test_split* method of scikit-learn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "X_train, X_test, y_train, y_test = train_test_split(df.text, df.airline_sentiment, test_size=0.1, random_state=37)\n",
      "print('# Train data samples:', X_train.shape[0])\n",
      "print('# Test data samples:', X_test.shape[0])\n",
      "assert X_train.shape[0] == y_train.shape[0]\n",
      "assert X_test.shape[0] == y_test.shape[0]"
     ],
     "language": "python",
     "metadata": {
      "_cell_guid": "df742ade-52d3-4b47-80b0-1f7cc26545eb",
      "_uuid": "6ab9296159749e4525244fae6d6f4e6a37121c93"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('# Train data samples:', 13176)\n",
        "('# Test data samples:', 1464)\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "07e3df8462df02ae8336a1b9123f928b8cce814d"
     },
     "source": [
      "### Converting words to numbers\n",
      "To use the text as input for a model, we first need to convert the tweet's words into tokens, which simply means converting the words to integers that refer to an index in a dictionary. Here we will only keep the most frequent words in the train set.\n",
      "\n",
      "We clean up the text by applying *filters* and putting the words to *lowercase*. Words are separated by spaces."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "tk = Tokenizer(num_words=NB_WORDS,\n",
      "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
      "               lower=True,\n",
      "               split=\" \")\n",
      "tk.fit_on_texts(X_train)\n",
      "\n",
      "print('Fitted tokenizer on {} documents'.format(tk.document_count))\n",
      "print('{} words in dictionary'.format(tk.num_words))\n",
      "print('Top 5 most common words are:', collections.Counter(tk.word_counts).most_common(5))"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "6cbf76b09ac27e67f7e92eb0986a963db00ddcd6"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitted tokenizer on 13176 documents\n",
        "10000 words in dictionary\n",
        "('Top 5 most common words are:', [('flight', 3556), ('not', 1440), ('no', 1372), ('get', 1213), ('t', 1096)])"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "a1bf813b1277821e18b257f207dfc8642ce5ad95"
     },
     "source": [
      "After having created the dictionary we can convert the text to a list of integer indexes. This is done with the *text_to_sequences* method of the Tokenizer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "X_train_seq = tk.texts_to_sequences(X_train)\n",
      "X_test_seq = tk.texts_to_sequences(X_test)\n",
      "print(X_train)\n",
      "##print('\"{}\" is converted into {}'.format(X_train[0], X_train_seq[0]))"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "28435e6639f4dac6d9bdc9df65b1dd4651b15617"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5164      went front restroom plane, sink sign said no ...\n",
        "291       posts second consecutive full-year net profit...\n",
        "5695      flying Southwest first time LGA layover Atlanta!\n",
        "637       July. You ZERO excuses this. You out-of-date ...\n",
        "422       thank you! See y'all soon! I'm excited see ex...\n",
        "14087                                Great seats aircraft!\n",
        "2893      appreciate sentiment able get ground;still mi...\n",
        "6224       . So important accept others are. #SWADiversity\n",
        "2355      upgrade first class nice way fix earlier mist...\n",
        "5268       could put one Baltimore? http://t.co/vLCI2KV1IP\n",
        "13590     delayed 15 hours Chicago still managed lose b...\n",
        "4665      Your Terry hero! Got husband back thru securi...\n",
        "2537      appreciate immediate offer, suspect long-term...\n",
        "477                               wifi AND better seating.\n",
        "10055      Dont bother. They dont pick phone. Worst cus...\n",
        "13998      surprise BA registered system getting hopefu...\n",
        "12675     biggest joke I've ever seen telling everyone ...\n",
        "14517     nah boofin dont talk like humans know. Respon...\n",
        "3811      Why take 4-6 weeks new MileagePlus Premier ca...\n",
        "9157      thanks however feel like system failed -- sol...\n",
        "2168      boarding decent, useless agents speak English...\n",
        "2362      ruined day &amp; start trip SFO GIG. legs cha...\n",
        "13766                                               thanks\n",
        "10551       i've hold hour trying change flight!!! COME ON\n",
        "12492     Thank acknowledgement. The IFE work well anyw...\n",
        "5085      holding hour 10 minutes. To rebook Cancelled ...\n",
        "8026      what's random delay flight 1729? Any chance f...\n",
        "13706     Your agent sent family diff planes dfw Late F...\n",
        "9165      I'm glad airline going swallowed American!!! ...\n",
        "8972                                        killing inside\n",
        "                               ...                        \n",
        "6026       birthday 24th he's not seeing Imagine Dragon...\n",
        "4678      Sorry bother you, I've hold hours 30 minutes....\n",
        "667       yep that's correct, got email 12:30 flight Ca...\n",
        "6880      delay flights. Then redirect gate. Then tell ...\n",
        "1002                   think actually not like screen  \ud83d\ude03\ud83d\ude03\ud83d\ude03\n",
        "13128     keep returning call hanging answer? Help reFl...\n",
        "2424      Friend O'Hare can't get flight bc say no proo...\n",
        "2161                   earned place worst airline business\n",
        "4108      Call customer service course say \"theres noth...\n",
        "3176      understand pilot would get plane 1st class? T...\n",
        "4899      love Companion Pass (qualified 4th year). #Ho...\n",
        "5052      sister&amp;brother law need get Florida despe...\n",
        "5839                                    please reply DM \u2665\ufe0f\n",
        "5294                                       follow send DM?\n",
        "12364                                          flight 5348\n",
        "6594      Received email w/reso's email associated SWA ...\n",
        "9954                                                 bags?\n",
        "3532      unavailable leg registered hours sold out. On...\n",
        "13913     say going help us? Give number call recording...\n",
        "2689                                            guys suck!\n",
        "14308                                             DM info?\n",
        "5526      happy finally put #southwestairlines boarding...\n",
        "13054     sure is. What's frustrating stranded airport ...\n",
        "9083                                    see board tomorrow\n",
        "45                    I'm #elevategold good reason: rock!!\n",
        "1348                                   child two years old\n",
        "9392                                          Late Flight.\n",
        "7064     \"LOL guys it\" me, months ago...\u201c: Our fleet's ...\n",
        "14042           bags arent even loaded plane??? #Flight293\n",
        "13322                                      check DM please\n",
        "Name: text, Length: 13176, dtype: object\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "0a6dbe6901ac0d7ce46797bd8d0d5c9722436c0b"
     },
     "source": [
      "These integers should now be converted into a one-hot encoded features."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def one_hot_seq(seqs, nb_features = NB_WORDS):\n",
      "    ohs = np.zeros((len(seqs), nb_features))\n",
      "    for i, s in enumerate(seqs):\n",
      "        ohs[i, s] = 1.\n",
      "    return ohs\n",
      "\n",
      "X_train_oh = one_hot_seq(X_train_seq)\n",
      "X_test_oh = one_hot_seq(X_test_seq)\n",
      "\n",
      "print('\"{}\" is converted into {}'.format(X_train_seq[0], X_train_oh[0]))\n",
      "print('For this example we have {} features with a value of 1.'.format(X_train_oh[0].sum()))"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "cd91541a0f3c027dca3429af497379fd86a7079b"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"[436, 715, 3481, 21, 6106, 893, 146, 3, 796, 201, 1336, 4336, 81, 13, 57, 92, 735, 131, 2092]\" is converted into [0. 0. 0. ... 0. 0. 0.]\n",
        "For this example we have 19.0 features with a value of 1.\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "0c35698c39d06b89151918b77c70791aac3fa63f"
     },
     "source": [
      "### Converting the target classes to numbers\n",
      "We need to convert the target classes to numbers as well, which in turn are one-hot-encoded with the *to_categorical* method in keras"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "le = LabelEncoder()\n",
      "y_train_le = le.fit_transform(y_train)\n",
      "y_test_le = le.transform(y_test)\n",
      "y_train_oh = to_categorical(y_train_le)\n",
      "y_test_oh = to_categorical(y_test_le)\n",
      "\n",
      "##print('\"{}\" is converted into {}'.format(y_train[0], y_train_le[0]))\n",
      "print('\"{}\" is converted into {}'.format(y_train_le[0], y_train_oh[0]))"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "f64570f56244464eaf88805a8c41b46f76db4668"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\"0\" is converted into [1. 0. 0.]\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "73b4fe31f03a45fff85e75b72141029416050d08"
     },
     "source": [
      "### Splitting of a validation set\n",
      "Now that our data is ready, we split of a validation set. This validation set will be used to evaluate the model performance when we tune the parameters of the model. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "X_train_rest, X_valid, y_train_rest, y_valid = train_test_split(X_train_oh, y_train_oh, test_size=0.1, random_state=37)\n",
      "\n",
      "assert X_valid.shape[0] == y_valid.shape[0]\n",
      "assert X_train_rest.shape[0] == y_train_rest.shape[0]\n",
      "\n",
      "print('Shape of validation set:',X_valid.shape)"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "e4b7683309d5501c80d9e4e7335a76e58e8999f0"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Shape of validation set:', (1318, 10000))\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {
      "_uuid": "8a52314ce6c1d0de5c3765ff5781b64c32c8871c"
     },
     "source": [
      "Deep learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "3e7e167507953da952e50257aae77f36ec28e6cc"
     },
     "source": [
      "### Baseline model\n",
      "We start with a model with 2 densely connected layers of 64 hidden elements. The *input_shape* for the first layer is equal to the number of words we allowed in the dictionary and for which we created one-hot-encoded features.\n",
      "\n",
      "As we need to predict 3 different sentiment classes, the last layer has 3 hidden elements. The *softmax* activation function makes sure the three probabilities sum up to 1.\n",
      "\n",
      "In the first layer we need to estimate 640064 weights. This is determined by (nb inputs * nb hidden elements) + nb bias terms, or (10000 x 64) + 64 = 640064<br>\n",
      "In the second layer we estimate (64 x 64) + 64 = 4160 weights<br>\n",
      "In the last layer we estimate (64 x 3) + 3 = 195 weights<br>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "base_model = models.Sequential()\n",
      "base_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
      "base_model.add(layers.Dense(64, activation='relu'))\n",
      "base_model.add(layers.Dense(3, activation='softmax'))\n",
      "base_model.summary()"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "343bddd22935a60332161c6ae70ba20d2e8087ed"
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "dense_1 (Dense)              (None, 64)                640064    \n",
        "_________________________________________________________________\n",
        "dense_2 (Dense)              (None, 64)                4160      \n",
        "_________________________________________________________________\n",
        "dense_3 (Dense)              (None, 3)                 195       \n",
        "=================================================================\n",
        "Total params: 644,419\n",
        "Trainable params: 644,419\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "2ec3b5ac6911389154d370037d853c257cc7c5fc"
     },
     "source": [
      "Because this project is a multi-class, single-label prediction, we use *categorical_crossentropy* as the loss function and *softmax* as the final activation function. We fit the model on the remaining train data and validate on the validation set. We run for a predetermined number of epochs and will see when the model starts to overfit."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def deep_model(model):\n",
      "    model.compile(optimizer='rmsprop'\n",
      "                  , loss='categorical_crossentropy'\n",
      "                  , metrics=['accuracy'])\n",
      "    \n",
      "    history = model.fit(X_train_rest\n",
      "                       , y_train_rest\n",
      "                       , epochs=NB_START_EPOCHS\n",
      "                       , batch_size=BATCH_SIZE\n",
      "                       , validation_data=(X_valid, y_valid)\n",
      "                       , verbose=0)\n",
      "    \n",
      "    return history"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "1be72007bccc687064f789f9564354329d32e102"
     },
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "base_history = deep_model(base_model)"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "c3ab89d1291d07550f403f8144af5b739821b6b6"
     },
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "df985523b02871ef30b4fa17a629b54fd090cb4d"
     },
     "source": [
      "To evaluate the model performance, we will look at the training and validation loss and accuracy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def eval_metric(history, metric_name):\n",
      "    metric = history.history[metric_name]\n",
      "    val_metric = history.history['val_' + metric_name]\n",
      "\n",
      "    e = range(1, NB_START_EPOCHS + 1)\n",
      "\n",
      "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
      "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
      "    plt.legend()\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "fe4399bd073104da7e8e1bf8b99f5ef8e57e93fa"
     },
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "3de1b2cf553848d8472ebca9d8156ec98eb231b2"
     },
     "source": [
      "We can see here that the validation loss starts to increase as from epoch 4. The training loss continues to lower, which is normal as the model is trained to fit the train data as good as possible."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "eval_metric(base_history, 'loss')"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "a8d0194eb405add560e62a66078e6145f12c2f36"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "b60582a3e6056ed34e980af793720fa9b5fb4906"
     },
     "source": [
      "Just as with the validation loss, the validation accuracy peaks at an early epoch. After that, it goes down slightly. So to conclude, we can say that the model starts overfitting as from epoch 4. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "eval_metric(base_history, 'acc')"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "150d375ca2a2642eec9199d5512fec19da13376d"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "0cc919808080166a9045d241e991f9a521e8abc8"
     },
     "source": [
      "### Handling overfitting\n",
      "Now, we can try to do something about the overfitting. There are different options to do that.\n",
      "* Option 1: reduce the network's size by removing layers or reducing the number of hidden elements in the layers\n",
      "* Option 2: add regularization, which comes down to adding a cost to the loss function for large weights\n",
      "* Option 3: adding dropout layers, which will randomly remove certain features by setting them to zero"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "6768d95a28ff456f33546660512f3ffb398a94d9"
     },
     "source": [
      "#### Reducing the network's size\n",
      "We reduce the network's size by removing one layer and lowering the number of hidden elements in the remaining layer to 32."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "reduced_model = models.Sequential()\n",
      "reduced_model.add(layers.Dense(32, activation='relu', input_shape=(NB_WORDS,)))\n",
      "reduced_model.add(layers.Dense(3, activation='softmax'))\n",
      "reduced_model.summary()"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "268a0b46d93dae85e0b536a69a3efdf6eaea402b"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "reduced_history = deep_model(reduced_model)"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "88dd2db6557cb7868db3a0f2caef8b45329c5fbb"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def compare_loss_with_baseline(h, model_name):\n",
      "    loss_base_model = base_history.history['val_loss']\n",
      "    loss_model = h.history['val_loss']\n",
      "\n",
      "    e = range(1, NB_START_EPOCHS + 1)\n",
      "\n",
      "    plt.plot(e, loss_base_model, 'bo', label='Validation Loss Baseline Model')\n",
      "    plt.plot(e, loss_model, 'b', label='Validation Loss ' + model_name)\n",
      "    plt.legend()\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "f7f430579f147e22312a6708d910ecdf67fbaf15"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "e4995437cae53991f1f54f9e9e0513147e28775f"
     },
     "source": [
      "We can see that it takes more epochs before the reduced model starts overfitting (around epoch 10). Moreover, the loss increases much slower after that epoch compared to the baseline model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "compare_loss_with_baseline(reduced_history, 'Reduced Model')"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "b68ea6fa1c73f2183528024631b0818393ddd8b8"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "bd65f73d772539b52c492ab60df5d07cf08a236b"
     },
     "source": [
      "#### Adding regularization\n",
      "To address overfitting, we can also add regularization to the model. Let's try with L2 regularization."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "reg_model = models.Sequential()\n",
      "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu', input_shape=(NB_WORDS,)))\n",
      "reg_model.add(layers.Dense(64, kernel_regularizer=regularizers.l2(0.001), activation='relu'))\n",
      "reg_model.add(layers.Dense(3, activation='softmax'))\n",
      "reg_model.summary()"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "17e384daee482847293925c5c98697eb93b10c81"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "reg_history = deep_model(reg_model)"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "b6eb4019580ea228350d1f143fbc9e3d34725c1b"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "8dff1fd40b74d2eed309964e6f4c4cbd6d11b098"
     },
     "source": [
      "For the regularized model we notice that it starts overfitting earlier than the baseline model. However, the loss increases much slower afterwards."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "compare_loss_with_baseline(reg_history, 'Regularized Model')"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "dba7e497c502649455e55209824b88ad6704a168"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "34c7ba86080b858475e08ae9652d9e7e1cb85dd4"
     },
     "source": [
      "#### Adding dropout layers\n",
      "The last option we'll try is to add dropout layers."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "drop_model = models.Sequential()\n",
      "drop_model.add(layers.Dense(64, activation='relu', input_shape=(NB_WORDS,)))\n",
      "drop_model.add(layers.Dropout(0.5))\n",
      "drop_model.add(layers.Dense(64, activation='relu'))\n",
      "drop_model.add(layers.Dropout(0.5))\n",
      "drop_model.add(layers.Dense(3, activation='softmax'))\n",
      "drop_model.summary()"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "148084e8fdbae9eb8aacb16f63fb5ad1afa715c7"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "drop_history = deep_model(drop_model)"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "68129f5c4e1408012bd5271c2e185a0f9b680fef"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "d56934aba84b08f01cbf2a1919809ee59eae3f48"
     },
     "source": [
      "The model with dropout layers starts overfitting a bit later than the baseline model. The loss also increases slower than the baseline model."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "compare_loss_with_baseline(drop_history, 'Dropout Model')"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "2f84d9a3b71665e31817b87c3f7e0a7dcda8f168"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "54e689e46bcb6f6c1697ce551c318cf1bc2f1826"
     },
     "source": [
      "### Training on the full train data and evaluation on test data\n",
      "At first sight the reduced model seems to be the best model for generalization. But let's check that on the test set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def test_model(model, epoch_stop):\n",
      "    model.fit(X_train_oh\n",
      "              , y_train_oh\n",
      "              , epochs=epoch_stop\n",
      "              , batch_size=BATCH_SIZE\n",
      "              , verbose=0)\n",
      "    results = model.evaluate(X_test_oh, y_test_oh)\n",
      "    \n",
      "    return results"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "8e820d0d52358f5a9a514c754bccd56cc0496cfe"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "base_results = test_model(base_model, 4)\n",
      "print('/n')\n",
      "print('Test accuracy of baseline model: {0:.2f}%'.format(base_results[1]*100))"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "55fee33d47ca1f488a5c4ca262d95e3edb873fda"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "reduced_results = test_model(reduced_model, 10)\n",
      "print('/n')\n",
      "print('Test accuracy of reduced model: {0:.2f}%'.format(reduced_results[1]*100))"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "fe5556d324cbef0b2f661772895ffca215313dcc"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "reg_results = test_model(reg_model, 5)\n",
      "print('/n')\n",
      "print('Test accuracy of regularized model: {0:.2f}%'.format(reg_results[1]*100))"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "ed5cddbc024532a609193637fdc7335c9ecd3ea2"
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "drop_results = test_model(drop_model, 6)\n",
      "print('/n')\n",
      "print('Test accuracy of dropout model: {0:.2f}%'.format(drop_results[1]*100))"
     ],
     "language": "python",
     "metadata": {
      "_uuid": "b6beaf7c1988d3bfcb24a03b70d2b5127a6ae38f"
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "_uuid": "7e70f3c62923a0d30b88789f841939c8abdc75c9"
     },
     "source": [
      "# Conclusion\n",
      "As we can see above, the model with the dropout layers performs the best on the test data. However, this is slightly lower than what we achieved with a LogisticRegression and Countvectorizer in my previous kernel. But there the input data was transformed a bit different than here. \n",
      "\n",
      "### What's next?\n",
      "Soon I will add the use of word embeddings in Keras and using a pretrained model. To be continued..."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}